{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing (NLP) using Python's NLTK\n",
    "\n",
    "One of the most frequent tasks in computational text analysis is quickly summarizing the content of text. In this lesson we will learn two ways of summarzing text using Python's nltk, and in the process learn some quick and easy NLP techniques.\n",
    "\n",
    "Natural Language Processing is an umbrella term that incorporates many techiques and methods to process, analyze, and understand natural languages (as opposed to artificial languages like logics, or Python).\n",
    "\n",
    "### Learning Goals:\n",
    "The goal of this lesson is to jump right in to text analysis and natural language processing. Rather than starting with the nitty gritty of programming in Python, this lesson will demonstrate some neat things you can do with a minimal amount of coding and will give you an understanding of why you may want to learn the nitty gritty. An additional goal of this lesson is to start to get you thinking about analyzing texts via computational methods.  <br /> <br />  By the end of the lesson you will learn how to quickly summarize a text via counting the most frequent words, nouns, and verbs. More specifically, you will:\n",
    "\n",
    "* Gain an intuition about how computers process text, and how this is different than how humans read it\n",
    "* Learn some of the basic functions in the nltk package, such as tokenizing texts and part-of-speech tagging, and learn why these might help researchers analyze text\n",
    "* Get started with some basic coding (although don't worry if you don't understand everything)\n",
    "\n",
    "\n",
    "### Lesson Outline:\n",
    "- Assigning Text as Variables in Python\n",
    "- Tokenizing Text and Counting Words\n",
    "- Pre-Processing: \n",
    "    * Changing words to lowercase\n",
    "    * Removing stop words\n",
    "    * Removing punctuation\n",
    "- Part-of-Speech Tagging\n",
    "    * Tagging tokens\n",
    "    * Counting tagged tokens\n",
    "- Illustration: Guess the Mystery Novels\n",
    "- If there's time: concordances\n",
    "\n",
    "\n",
    "### Key Jargon:\n",
    "* *coding or programming*: \n",
    "    * The purpose of programming is to find a sequence of instructions that will enable a computer to perform a specific task or solve a given problem. It involves writing those instructions in a specific *programming language*, in our case, Python.\n",
    "* *script*:\n",
    "    * A block of executable code, typically saved in a executable file. For example, script1.py\n",
    "* *packages and modules*: \n",
    "    * Python files, or collections of files, that implement a set of pre-made functions (so we don't have to write all of the functions ourselves). To utilize a module we use the import function.\n",
    "* *parse*: \n",
    "    * the process of analysing a string of symbols, in this case the symbols that make up natural language. This can also include understanding, or parsing, computer code.\n",
    "* *variable*: \n",
    "    * A variable is something that holds a value that may change. In simplest terms, a variable is just a box that you can put stuff in. You can use variables to store all kinds of stuff, including numbers and letters.\n",
    "* *assigning a variable*: \n",
    "    * telling Python what you want to name the variable, and what is stored in the variable.\n",
    "* *string*: \n",
    "    * a type of variable the consists of a sequence of characters in a particular order. Characters can be anything, including letters or numbers. The order of a string is fixed.\n",
    "* *list*: \n",
    "    * a type of variable that consists of a sequence of elements. The order is fixed.\n",
    "* *stop words*: \n",
    "    * the most common words in a language.\n",
    "\n",
    "### Further Resources:\n",
    "\n",
    "Check out the full range of techniques included in Python's nltk package here: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Assigning Text as a Variable in Python\n",
    "\n",
    "First, we assign a sample sentence, our \"text\", to a variable called \"sentence\" (the name of the variable is arbitrary). Printing the sentence shows what the variable \"sentence\" contains. \n",
    "\n",
    "Note: This sentence is a quote about what digital humanities means, from digital humanist Kathleen Fitzpatrick. Source: \"On Scholarly Communication and the Digital Humanities: An Interview with Kathleen Fitzpatrick\", *In the Library with the Lead Pipe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study. And that happens in two different ways. On the one hand, it’s bringing the tools and techniques of digital media to bear on traditional humanistic questions; on the other, it’s also bringing humanistic modes of inquiry to bear on digital media.\n"
     ]
    }
   ],
   "source": [
    "#Anything on a line starting with a hashtag is called a comment, and is meant to clarify code for human readers.\n",
    "#The computer ignores these lines.\n",
    "\n",
    "#assign the desired sentence to the variable called 'sentence.' This variable type is called a string.\n",
    "sentence = \"For me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study. And that happens in two different ways. On the one hand, it’s bringing the tools and techniques of digital media to bear on traditional humanistic questions; on the other, it’s also bringing humanistic modes of inquiry to bear on digital media.\"\n",
    "\n",
    "#print the contents of the variable 'sentence'\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'me', 'it', 'has', 'to', 'do', 'with', 'the', 'work', 'that', 'gets', 'done', 'at', 'the', 'crossroads', 'of', 'digital', 'media', 'and', 'traditional', 'humanistic', 'study', '.', 'And', 'that', 'happens', 'in', 'two', 'different', 'ways', '.', 'On', 'the', 'one', 'hand', ',', 'it’s', 'bringing', 'the', 'tools', 'and', 'techniques', 'of', 'digital', 'media', 'to', 'bear', 'on', 'traditional', 'humanistic', 'questions', ';', 'on', 'the', 'other', ',', 'it’s', 'also', 'bringing', 'humanistic', 'modes', 'of', 'inquiry', 'to', 'bear', 'on', 'digital', 'media', '.']\n"
     ]
    }
   ],
   "source": [
    "#First import the Python package nltk (Natural Language Tool Kit)\n",
    "import nltk\n",
    "\n",
    "#import the function to split the text into separate words from the NLTK package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#create new variable that applies the word_tokenize function to our sentence.\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "#This new variable contains the tokenized text, and is now a variable type called a list.\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Tokenizing Text and Counting Words\n",
    "\n",
    "The above output is how a human would read that sentence. Next we look at different ways in which a computer \"reads\", or *parses*, that sentence, and some simple ways to analyze/summarize it.\n",
    "\n",
    "Often the first step needed to enable a computer to parse text is changing the sentence into \"tokens.\" This is referred to as *tokenizing* text. Each token roughly corresponds to either words or punctuation. In essence, this process divides the sentence into little bits that the computer can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice each token is either a word or punctuation. [Note: in the coming days we will see other methods to tokenize text. While seemingly simple, tokenizing text is not a trivial task.]\n",
    "\n",
    "Why is this helpful?\n",
    "\n",
    "We can now summarize the sentence/text in interesting and potentially helpful ways. For example, we can count the number of tokens in the sentence, which roughly corresponds to the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "#The number of tokens is the length of the list, or the number of elements in the list\n",
    "print(len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the most frequent words, which can help us quickly summarize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5), ('humanistic', 3), ('to', 3), ('digital', 3), ('.', 3), ('on', 3), ('of', 3), ('media', 3), ('it’s', 2), ('and', 2)]\n"
     ]
    }
   ],
   "source": [
    "#apply the nltk function FreqDist to count the number of times each token occurs.\n",
    "word_frequency = nltk.FreqDist(sentence_tokens)\n",
    "\n",
    "#print out the 10 most frequent words using the function most_common\n",
    "print(word_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words do suggest what the sentence is about, in particular the words \"humanistic\", \"digital\", \"media\", and \"traditional\".\n",
    "\n",
    "But there are many frequent words that are not helpful in summarizing the text, for example, \"the\", \"and\", \"to\", and \".\" So the most frequent words do not necessarily help us understand the content of a text.\n",
    "\n",
    "How can we use a computer to identify important, interesting, or content words in a text? There are many ways to do this, a few of which we'll cover in this workshop. Today, we'll look at two simple ways to identify words that will help us summarize the content of a text. We'll see additional ways of doing this throughout the week. \n",
    "\n",
    "### 2. Pre-Processing: Lower Case, Removing Stop Words and Punctuation\n",
    "\n",
    "First, scholars typically go through a number of pre-processing steps before getting to the actual analysis. One of these step is converting all words to lower-case, so that the word \"Humanities\" and \"humanities\" count as the same word. (For some tasks this is appropriate. Think of reasons why we might NOT want to do this.)\n",
    "\n",
    "To convert to lower case we use the function lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'me', 'it', 'has', 'to', 'do', 'with', 'the', 'work', 'that', 'gets', 'done', 'at', 'the', 'crossroads', 'of', 'digital', 'media', 'and', 'traditional', 'humanistic', 'study', '.', 'and', 'that', 'happens', 'in', 'two', 'different', 'ways', '.', 'on', 'the', 'one', 'hand', ',', 'it’s', 'bringing', 'the', 'tools', 'and', 'techniques', 'of', 'digital', 'media', 'to', 'bear', 'on', 'traditional', 'humanistic', 'questions', ';', 'on', 'the', 'other', ',', 'it’s', 'also', 'bringing', 'humanistic', 'modes', 'of', 'inquiry', 'to', 'bear', 'on', 'digital', 'media', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens_lc = [word.lower() for word in sentence_tokens]\n",
    "\n",
    "#see the result\n",
    "print(sentence_tokens_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words like \"the\", \"to\", and \"and\" are what text analysis call \"stop words.\" Stop words are the most common words in a language, and while necessary and useful for some analysis purposes, do not tell us much about the *substance* of a text. Another common pre-processing steps is to simply remove punctuation and stop words. NLTK contains a built-in stop words list, which we use to remove stop words from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "#import the stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#take a look at what stop words are included:\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'gets', 'done', 'crossroads', 'digital', 'media', 'traditional', 'humanistic', 'study', '.', 'happens', 'two', 'different', 'ways', '.', 'one', 'hand', ',', 'it’s', 'bringing', 'tools', 'techniques', 'digital', 'media', 'bear', 'traditional', 'humanistic', 'questions', ';', ',', 'it’s', 'also', 'bringing', 'humanistic', 'modes', 'inquiry', 'bear', 'digital', 'media', '.']\n"
     ]
    }
   ],
   "source": [
    "#create a new variable that contains the sentence tokens without the stopwords\n",
    "sentence_tokens_clean = [word for word in sentence_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "#see what words we're left with\n",
    "print(sentence_tokens_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation also does not help us understand the substance of a text, so we'll remove punctuation in a similar fashion. [Again, think about tasks where me may not want to remove punctuation.] There are many many ways to do this. For now, we'll create a list of punctuation tokens, similar to the list of stop words, and remove them from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'gets', 'done', 'crossroads', 'digital', 'media', 'traditional', 'humanistic', 'study', 'happens', 'two', 'different', 'ways', 'one', 'hand', 'it’s', 'bringing', 'tools', 'techniques', 'digital', 'media', 'bear', 'traditional', 'humanistic', 'questions', 'it’s', 'also', 'bringing', 'humanistic', 'modes', 'inquiry', 'bear', 'digital', 'media']\n"
     ]
    }
   ],
   "source": [
    "#creat list of punctuation symbols\n",
    "#there are better ways to do this which we'll get to later, but we'll keep it simple here\n",
    "punctuation = [\".\", \";\", \",\", \"'\", '\"', \"!\"]\n",
    "sentence_tokens_clean = [word for word in sentence_tokens_clean if word not in punctuation]\n",
    "\n",
    "#see what's left\n",
    "print(sentence_tokens_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after our pre-processing steps, let's re-count the most frequent words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('digital', 3), ('humanistic', 3), ('media', 3), ('it’s', 2), ('bear', 2), ('traditional', 2), ('bringing', 2), ('study', 1), ('techniques', 1), ('questions', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_frequency_clean = nltk.FreqDist(sentence_tokens_clean)\n",
    "print(word_frequency_clean.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! The 10 most frequent words now give us a pretty good sense of the substance of this sentence. But we still have problems. For example, the word \"it's\" sneaked in there. One solution is to keep adding stop words to our stop word list, but this could go on forever and is not a good solution when processing lots of text.\n",
    "\n",
    "There's another way of identifying content words, and it involves identifying the part of speech of each word.\n",
    "\n",
    "### 3. Part-of-Speech Tagging\n",
    "\n",
    "You may have noticed that stop words are typically short function words. Intuitively, if we could identify the part of speech of a word, we would have another way of identifying words of substance. NLTK can do that too!\n",
    "\n",
    "NLTK has a function that will tag the part of speech of every token in a text. For this, we go back to our original tokenized text, with the stop words and punctuation.\n",
    "\n",
    "NLTK uses the Penn Treebank Project to tag the part-of-speech of the words. You can find a list of all the part-of-speech tags here:\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('For', 'IN'), ('me', 'PRP'), ('it', 'PRP'), ('has', 'VBZ'), ('to', 'TO'), ('do', 'VB'), ('with', 'IN'), ('the', 'DT'), ('work', 'NN'), ('that', 'WDT'), ('gets', 'VBZ'), ('done', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('crossroads', 'NNS'), ('of', 'IN'), ('digital', 'JJ'), ('media', 'NNS'), ('and', 'CC'), ('traditional', 'JJ'), ('humanistic', 'JJ'), ('study', 'NN'), ('.', '.'), ('And', 'CC'), ('that', 'DT'), ('happens', 'VBZ'), ('in', 'IN'), ('two', 'CD'), ('different', 'JJ'), ('ways', 'NNS'), ('.', '.'), ('On', 'IN'), ('the', 'DT'), ('one', 'CD'), ('hand', 'NN'), (',', ','), ('it’s', 'NN'), ('bringing', 'VBG'), ('the', 'DT'), ('tools', 'NNS'), ('and', 'CC'), ('techniques', 'NNS'), ('of', 'IN'), ('digital', 'JJ'), ('media', 'NNS'), ('to', 'TO'), ('bear', 'VB'), ('on', 'IN'), ('traditional', 'JJ'), ('humanistic', 'JJ'), ('questions', 'NNS'), (';', ':'), ('on', 'IN'), ('the', 'DT'), ('other', 'JJ'), (',', ','), ('it’s', 'NN'), ('also', 'RB'), ('bringing', 'VBG'), ('humanistic', 'JJ'), ('modes', 'NNS'), ('of', 'IN'), ('inquiry', 'NN'), ('to', 'TO'), ('bear', 'VB'), ('on', 'IN'), ('digital', 'JJ'), ('media', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#use the nltk pos function to tag the tokens\n",
    "tagged_sentence_tokens = nltk.pos_tag(sentence_tokens)\n",
    "\n",
    "#view new variable\n",
    "print(tagged_sentence_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes more complicated code. Stay with me, but focus more on the output and understanding ways in which you as a researcher can use the output, rather than understanding every line of code.\n",
    "\n",
    "We can count the part-of-speech tags in a similar way we counted words, to output the most frequent types of words in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IN', 11),\n",
       " ('JJ', 10),\n",
       " ('NNS', 9),\n",
       " ('DT', 6),\n",
       " ('NN', 6),\n",
       " ('VB', 3),\n",
       " ('.', 3),\n",
       " ('TO', 3),\n",
       " ('CC', 3),\n",
       " ('VBZ', 3),\n",
       " (',', 2),\n",
       " ('VBG', 2),\n",
       " ('CD', 2),\n",
       " ('PRP', 2),\n",
       " (':', 1),\n",
       " ('VBN', 1),\n",
       " ('WDT', 1),\n",
       " ('RB', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_frequency = nltk.FreqDist(tag for (word, tag) in tagged_sentence_tokens)\n",
    "tagged_frequency.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence contains a lot of adjectives. So let's first look at the most frequent adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['digital', 'traditional', 'humanistic', 'different', 'digital', 'traditional', 'humanistic', 'other', 'humanistic', 'digital']\n"
     ]
    }
   ],
   "source": [
    "adjectives = [word for word,pos in tagged_sentence_tokens if pos == 'JJ' or pos=='JJR' or pos=='JJS']\n",
    "\n",
    "#print all of the adjectives\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('humanistic', 3), ('digital', 3), ('traditional', 2), ('other', 1), ('different', 1)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency of the adjectives\n",
    "freq_adjectives=nltk.FreqDist(adjectives)\n",
    "\n",
    "#print the most frequent adjectives\n",
    "print(freq_adjectives.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'crossroads', 'media', 'study', 'ways', 'hand', 'it’s', 'tools', 'techniques', 'media', 'questions', 'it’s', 'modes', 'inquiry', 'media']\n"
     ]
    }
   ],
   "source": [
    "nouns = [word for word,pos in tagged_sentence_tokens if pos=='NN' or pos=='NNS']\n",
    "\n",
    "#print all of the nouns\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('media', 3), ('it’s', 2), ('inquiry', 1), ('hand', 1), ('work', 1)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency of the nouns\n",
    "freq_nouns=nltk.FreqDist(nouns)\n",
    "\n",
    "#print the most frequent nouns\n",
    "print(freq_nouns.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has', 'do', 'gets', 'done', 'happens', 'bringing', 'bear', 'bringing', 'bear']\n"
     ]
    }
   ],
   "source": [
    "verbs = [word for word,pos in tagged_sentence_tokens if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "\n",
    "#print all of the verbs\n",
    "print(verbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bringing', 2), ('bear', 2), ('gets', 1), ('has', 1), ('done', 1)]\n"
     ]
    }
   ],
   "source": [
    "#calculate the frequency of the nouns\n",
    "freq_verbs=nltk.FreqDist(verbs)\n",
    "\n",
    "#print the most frequent nouns\n",
    "print(freq_verbs.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we bring all of this together we get a pretty good summary of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('humanistic', 3), ('digital', 3), ('traditional', 2)]\n",
      "[('media', 3), ('it’s', 2), ('inquiry', 1)]\n",
      "[('bringing', 2), ('bear', 2), ('gets', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(freq_adjectives.most_common(3))\n",
    "print(freq_nouns.most_common(3))\n",
    "print(freq_verbs.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Illustration: Guess the Mystery Novel\n",
    "\n",
    "To illustrate this process on a slightly larger scale, we will do the exactly what we did above, but will do so on two mystery novels. Your challenge: guess the novels from the most frequent words, nouns, and verbs. We will do this in one chunk of code, so another challenge for you during breaks or the next few weeks is to see how much of the following code you can follow (or, in computer science terms, how much of the code you can parse). If the answer is none, not to worry! Tomorrow we will take a step back and work on the nitty gritty of programming.\n",
    "\n",
    "Note: this codes requires one thing we haven't covered: reading a .txt file from your hard drive. We'll go over this again in the upcoming days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "#import the package 'string' for a different way of removing punctuation. It's simply a more complete list of punction than we created above.\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "#see what punctuation is included\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the two text files from your hard drive, assign first mystery text to variable 'text1' and second mystery text to variable 'text2'\n",
    "text1 = open('text1.txt').read()\n",
    "text2 = open('text2.txt').read()\n",
    "\n",
    "###word frequencies\n",
    "\n",
    "#tokenize texts\n",
    "text1_tokens = word_tokenize(text1)\n",
    "text2_tokens = word_tokenize(text2)\n",
    "\n",
    "#pre-process for word frequency\n",
    "#lowercase\n",
    "text1_tokens_lc = [word.lower() for word in text1_tokens]\n",
    "text2_tokens_lc = [word.lower() for word in text2_tokens]\n",
    "\n",
    "#remove stopwords\n",
    "text1_tokens_clean = [word for word in text1_tokens_lc if word not in stopwords.words('english')]\n",
    "text2_tokens_clean = [word for word in text2_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "#remove punctuation using the list of punctuation from the string pacage\n",
    "text1_tokens_clean = [word for word in text1_tokens_clean if word not in punctuations]\n",
    "text2_tokens_clean = [word for word in text2_tokens_clean if word not in punctuations]\n",
    "\n",
    "#frequency distribution\n",
    "text1_word_frequency = nltk.FreqDist(text1_tokens_clean)\n",
    "text2_word_frequency = nltk.FreqDist(text2_tokens_clean)\n",
    "\n",
    "###part-of-speech frequencies\n",
    "\n",
    "#tag part-of-speech\n",
    "text1_tagged = nltk.pos_tag(text1_tokens)\n",
    "text2_tagged = nltk.pos_tag(text2_tokens)\n",
    "\n",
    "#most frequent nouns and verbs\n",
    "text1_nouns = [word for word,pos in text1_tagged if pos=='NN' or pos=='NNS']\n",
    "text2_nouns = [word for word,pos in text2_tagged if pos=='NN' or pos=='NNS']\n",
    "text1_freq_nouns=nltk.FreqDist(text1_nouns)\n",
    "text2_freq_nouns=nltk.FreqDist(text2_nouns)\n",
    "\n",
    "text1_verbs = [word for word,pos in text1_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "text2_verbs = [word for word,pos in text2_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "\n",
    "text1_freq_verbs=nltk.FreqDist(text1_verbs)\n",
    "text2_freq_verbs=nltk.FreqDist(text2_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables are assigned as desired. We can now print out the most frequent words, nouns, and verbs for each text. Again, don't worry if you don't understand all the code here. [Note: many of these pring statements are so humans can better read the output.]\n",
    "\n",
    "Can you guess the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent words for Text1:\n",
      "_________________________\n",
      "'s\n",
      "''\n",
      "``\n",
      "whale\n",
      "one\n",
      "like\n",
      "upon\n",
      "ahab\n",
      "man\n",
      "ship\n",
      "old\n",
      "would\n",
      "ye\n",
      "sea\n",
      "though\n",
      "yet\n",
      "time\n",
      "captain\n",
      "long\n",
      "still\n",
      "\n",
      "Frequent nouns for Text1\n",
      "________________________\n",
      "whale\n",
      "man\n",
      "ship\n",
      "sea\n",
      "time\n",
      "boat\n",
      "head\n",
      "way\n",
      "whales\n",
      "men\n",
      "hand\n",
      "thing\n",
      "side\n",
      "ye\n",
      "world\n",
      "water\n",
      "deck\n",
      "day\n",
      "eyes\n",
      "sort\n",
      "\n",
      "Frequent verbs for Text1\n",
      "________________________\n",
      "is\n",
      "was\n",
      "be\n",
      "had\n",
      "have\n",
      "were\n",
      "are\n",
      "'s\n",
      "been\n",
      "do\n",
      "said\n",
      "has\n",
      "seemed\n",
      "did\n",
      "see\n",
      "say\n",
      "being\n",
      "go\n",
      "made\n",
      "seen\n",
      "\n",
      "------------------------\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "------------------------\n",
      "\n",
      "Frequent words for Text2\n",
      "________________________\n",
      "''\n",
      "``\n",
      "'s\n",
      "elinor\n",
      "could\n",
      "marianne\n",
      "mrs.\n",
      "would\n",
      "said\n",
      "every\n",
      "one\n",
      "much\n",
      "must\n",
      "sister\n",
      "edward\n",
      "dashwood\n",
      "mother\n",
      "time\n",
      "jennings\n",
      "know\n",
      "\n",
      "Frequent nouns for Text2\n",
      "________________________\n",
      "sister\n",
      "mother\n",
      "time\n",
      "thing\n",
      "nothing\n",
      "house\n",
      "day\n",
      "heart\n",
      "man\n",
      "moment\n",
      "room\n",
      "mind\n",
      "kind\n",
      "world\n",
      "town\n",
      "morning\n",
      "family\n",
      "affection\n",
      "brother\n",
      "place\n",
      "\n",
      "Frequent verbs for Text2\n",
      "________________________\n",
      "was\n",
      "be\n",
      "had\n",
      "have\n",
      "is\n",
      "been\n",
      "were\n",
      "said\n",
      "do\n",
      "am\n",
      "know\n",
      "are\n",
      "did\n",
      "think\n",
      "has\n",
      "see\n",
      "being\n",
      "say\n",
      "make\n",
      "made\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequent words for Text1:\")\n",
    "print(\"_________________________\")\n",
    "\n",
    "for word in text1_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent nouns for Text1\")\n",
    "print(\"________________________\")\n",
    "for word in text1_freq_nouns.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent verbs for Text1\")\n",
    "print(\"________________________\")\n",
    "for word in text1_freq_verbs.most_common(20):\n",
    "    print(word[0])\n",
    "    \n",
    "print()\n",
    "print(\"------------------------\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Frequent words for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "\n",
    "print()\n",
    "print(\"Frequent nouns for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_freq_nouns.most_common(20):\n",
    "    print(word[0])\n",
    "\n",
    "print()\n",
    "print(\"Frequent verbs for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_freq_verbs.most_common(20):\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What further things can we learn from these lists? In particular, what can we learn from comparing these two novels?<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One thing I noticed: the verbs \"think\" and \"know\" are frequent verbs in Text2 but not Text1. Does this generate a potential research question?\n",
    "\n",
    "What next steps would you want to take if you were to further compare these novels?\n",
    "\n",
    "We can do many more things with the Python package NLTK. Pending time, here are a few other tricks.\n",
    "\n",
    "### 5. Concordances and Similar Words using NLTK\n",
    "\n",
    "Maybe we don't just want to know frequent words, but we want to know the way specific words are used. Concordances show us every occurrence of a given word, together with some context. This, combined with a function that should which words are used in a similar context as a given word, can help us understand the way in which a word is used in a text. \n",
    "\n",
    "To illustrate this, we can compare the way the word \"monstrous\" is used in our two novels. To use some nltk functions on text, for example concordances, we need to first transform the text into an NLTK text object. This will be useful for using these functions on your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: ETYMOLOGY . ( Supplied by a Late Consumptive...>\n",
      "<Text: CHAPTER 1 The family of Dashwood had long...>\n"
     ]
    }
   ],
   "source": [
    "#first tokenize the two texts\n",
    "text1_tokens = word_tokenize(text1)\n",
    "text2_tokens = word_tokenize(text2)\n",
    "\n",
    "#then transform the tokenized text into an NLTK text object\n",
    "text1_nltk = nltk.Text(text1_tokens)\n",
    "text2_nltk = nltk.Text(text2_tokens)\n",
    "\n",
    "#the variables text1_nltk and text2_nltk are now nltk text objects:\n",
    "print(text1_nltk)\n",
    "print(text2_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size ... . This came towards us , \n",
      "n of the Psalms . `` Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      " Radney . ' '' CHAPTER 55 . Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "e of Whale-Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n",
      "\n",
      "Displaying 11 of 11 matches:\n",
      " `` Now , Palmer , you shall see a monstrous pretty girl . '' He immediately we\n",
      "your sister is to marry him . I am monstrous glad of it , for then I shall have\n",
      "ou may tell your sister . She is a monstrous lucky girl to get him , upon my ho\n",
      "k how you will like them . Lucy is monstrous pretty , and so good humoured and \n",
      "Jennings , `` I am sure I shall be monstrous glad of Miss Marianne 's company ,\n",
      "usual noisy cheerfulness , `` I am monstrous glad to see you—sorry I could not \n",
      "t however , as it turns out , I am monstrous glad there was never any thing in \n",
      "so scornfully ! for they say he is monstrous fond of her , as well he may . I s\n",
      "le that she should be . '' `` I am monstrous glad of it . Good gracious ! I hav\n",
      "thing of the kind . So then he was monstrous happy , and talked on some time ab\n",
      "e very genteel people . He makes a monstrous deal of money , and they keep thei\n"
     ]
    }
   ],
   "source": [
    "#now we can use the concordance function to display the word in its context\n",
    "text1_nltk.concordance(\"monstrous\")\n",
    "print()\n",
    "text2_nltk.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk function *similar* prints out words that are used in the same context as monstrous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melville\n",
      "candid maddens fearless horrible trustworthy doleful part imperial\n",
      "determined untoward few passing vexatious tyrannical loving christian\n",
      "true lamentable impalpable curious\n",
      "\n",
      "Austen\n",
      "very so heartily exceedingly great a remarkably as extremely good\n",
      "sweet vast amazingly\n"
     ]
    }
   ],
   "source": [
    "print(\"Melville\")\n",
    "text1_nltk.similar(\"monstrous\")\n",
    "print()\n",
    "print(\"Austen\")\n",
    "text2_nltk.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn from this? For me, Melville uses \"monstrous\" in a mix of settings, but in many cases monstrous has a negative connotation. For Austen, monstrous has a positive connotation, and is often an amplifier of \"very\".\n",
    "\n",
    "Traditional literary criticism (according to an expert colleague of mine), has claimed that monstrous is a positive term for Melville, used in an admiring way, admiring of the power and enormity of whales. But this analysis suggests this may not be entirely true. Were literary critics wrong about the tone of the novel? Or partially right, but they missed something as well? What further questions could we ask or analyses could we do to explore this contention further?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
